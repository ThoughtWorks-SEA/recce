micronaut:
  application:
    name: recce
  router:
    static-resources:
      swagger:
        paths: classpath:META-INF/swagger
        mapping: /swagger/**
      rapidoc:
        paths: classpath:META-INF/swagger/views/rapidoc
        mapping: /rapidoc/**
  security:
    intercept-url-map:
      - pattern: /swagger/**
        access:
          - isAnonymous()
      - pattern: /rapidoc/**
        access:
          - isAnonymous()
logger:
  levels:
    io.micronaut.context.env: DEBUG # Log by default how properties are being loaded at startup
    io.micronaut.data: INFO # Increase logging level to see what is happening on Recce's own DB
    io.r2dbc.pool: INFO # Increase logging level to monitor connection usage from pools
database:
  host: localhost
  port: 9000
  name: db
  username: user
  password: password
datasources:
  default:
    url: jdbc:postgresql://${database.host}:${database.port}/${database.name}
    driverClassName: org.postgresql.Driver
    username: ${database.username}
    password: ${database.password}
    initialization-fail-timeout: 10000
    # Only need a small pool; as only should be required for migration during startup if we continue to use R2DBC
    maximum-pool-size: 2
    minimum-idle: 0
flyway:
  datasources:
    default:
      enabled: true
r2dbc:
  datasources:
    default:
      url: r2dbc:pool:postgresql://${database.host}:${database.port}/${database.name}
      username: ${database.username}
      password: ${database.password}
      options:
        maxSize: 10
        maxAcquireTime: PT60S
jackson:
  serialization:
    writeDatesAsTimestamps: false
auth:
  # HTTP Basic auth username for access to Recce APIs
  username: ""
  # HTTP Basic auth password for access to Recce APIs
  password: ""
reconciliation:
  # Configuration that applies to all data set definitions by default. Some can be overridden dataset-by-dataset.
  defaults:
    # How many rows are bulk inserted/updated When bulk loading data from your source & target DBs into Recce's DB.
    #
    # When loading target data, note that this represents an upper limit of each batch, since rows must be split into
    # updates (to an existing row's MigrationKey, loaded from the source) and inserts for rows that are missing in source.
    batchSize: 1000

    # Number of batches to potentially be saving in parallel when loading
    #
    # This imposes an upper limit on the number of Recce DB connections an individual dataset's reconciliation
    # run can be using in order to batch-insert/save rows. Excessive batch concurrency relative to available DB connections
    # may lead to R2DBC connection pool exhaustion/waits, and push excessive load to your Recce DB. In other words,
    # increasing concurrency beyond `r2dbc.datasources.default.options.maxSize` is unlikely to increase overall speed.
    batchConcurrency: 5

    # Approach to hashing values from columns in source and target data sources.
    #
    # A `TypeStrict` strategy will generally consider numbers of different size different to one another depending
    # on the type returned from the relevant database's driver. e.g if one datasource returns a 32-bit integer,
    # and another returns a 64-bit long a strict strategy will consider these values different, even if they have the
    # same numerical value.
    # A `TypeLenient` strategy will generally consider these values equal regardless of the types, at the expense of
    # coercing everything to it's largest size type when hashing. Note that this also means that a BOOLEAN of value 1
    # will also generally be considered equal to an INTEGER or NUMBER of value 1. The lenient strategy is especially
    # recommended when comparing databases of different types, since there are likely differences in how the drivers
    # treat types, even ignoring the underlying differences in types specified in tables.
    hashingStrategy: TypeLenient

    # Directory where queries for datasets may be loaded
    #
    # For each dataset source/target, if both `query` and `queryFile` are not specified,
    # load the query statement from ${dataset-name}-${source-or-target}.sql in this directory.
    queryFileBaseDir: queries

  datasets:
#    my-data-set: # Name your datasets however you would like
#      source:
#        # Reference to a datasource defined in `r2dbc.datasources`
#        datasourceRef: my-source-db
#        # Specify whether to load inline query or read from a query file
#        # If both `query` and `queryFile` are provided, inline query takes precedence
#        # If both query and queryFile are not specified,
#        # load the query statement from ${dataset-name}-${source-or-target}.sql in `query-file-base-dir`
#        # Optional SQL query to evaluate against the source DB
#        query: >
#          SELECT id AS MigrationKey, * FROM my-table
#        # Optional path to file containing query to evaluate against the source DB
#        queryFile: examples/scenario/<{example-scenario}/queries/<{example-scenario}-my-data-set-source.yml
#      target:
#        # Reference to a datasource defined in `r2dbc.datasources`
#        datasourceRef: my-target-db
#        # Specify whether to load inline query or read from a query file
#        # If both `query` and `queryFile` are provided, inline query takes precedence
#        # If both query and queryFile are not specified,
#        # load the query statement from ${dataset-name}-${source-or-target}.sql in `query-file-base-dir`
#        # Optional SQL query to evaluate against the target DB
#        query: >
#          SELECT id AS MigrationKey, * FROM my-table
#        # Optional path to file containing query to evaluate against the target DB
#        queryFile: examples/scenario/<{example-scenario}/queries/<{example-scenario}-my-data-set-target.yml
#      # Optional strategy for determining equivalence of column values
#      hashingStrategy: TypeLenient
#      # Optional scheduling of regular or one-of reconciliations
#      schedule:
#        # Must adhere to format https://docs.micronaut.io/latest/api/io/micronaut/scheduling/cron/CronExpression.html
#        # or https://crontab.guru/ (without seconds)
#        cronExpression: 0 0 * * *
